import os
from multiprocessing.pool import Pool
import requests
from bs4 import BeautifulSoup


def get_html_text_with_get(url,timeout=5):
    # 模拟get请求，获取页面
    try:
        r = requests.get(url)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return r.text
    except:
        return "error"


def get_html_text_with_post(url,timeout=5):
    #模拟post请求，获取页面
    try:
        r = requests.post(url)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return r.text
    except:
        return "error"


def parse_url_list(url):

    url_list = []
    html = get_html_text_with_post(url)
    if html != "error":
        soup = BeautifulSoup(html,"lxml")
        urls = soup.find_all("a",class_="list-link")
        for url in urls:
            url_list.append("http:" + url["href"])
    else:
        print("发生错误")

    return url_list


def parse_img_package(url):

    image_list = []
    html = get_html_text_with_get(url)
    if html != "error":
        soup = BeautifulSoup(html,"lxml")

        name = soup.find("div",class_="diary-data").span.text
        basediv = soup.find("div",class_="list-imgs")
        for img in basediv:
            urls = img["data-src"].split("//")
            for url in urls:
                if len(url) > 0:
                    image_list.append("http://" + url.replace(",", ""))

        print("相册:{} 解析完毕".format(name))
        return dict(urls=image_list, name=name)


def img_downloader(package):

    dirname = BASE_DIR + "imgs/" + package["name"]

    #创建对应的文件夹
    if not os.path.exists(dirname):
        os.mkdir(dirname)

        for url in package["urls"]:
            filename = url.split("/")[-1]
            open(dirname + "/" + filename, "wb").write(requests.get(url).content)
            print("正在下载:{}".format(filename))
        print("{}相册下载完毕!".format(package["name"]))
    else:
        print("该相册已经下载过了")


url_list = ["http://note.yuemei.com/chest/p{}.html".format(i for i in range(1,51))]

BASE_DIR = os.path.dirname(os.path.abspath(__file__)) + "/"


if __name__ == '__main__':

    # 初始化数据容器
    packages = []

    for url in url_list[5:10]:
        diarys = parse_url_list(url)

        for diary_url in diarys:
            #解析package，并存入列表
            package = parse_img_package(diary_url)
            if package != "error":
                packages.append(package)

    #开启多进程模式
    pool = Pool()
    pool.map(img_downloader, packages)
    pool.close()
    pool.join()

